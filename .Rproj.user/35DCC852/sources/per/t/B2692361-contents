---
title: "Analisis de clusters"
output: github_document
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Para el analisis de clusters vamos a analizar data de ventas de videojuegos, echaremos un vistazo a las variables presentes.
```{r load data, message= FALSE}
library(tidyverse)

data  <- read.csv("video_games_sales.csv")

summary(data)
```

Para clusterizar vamos a seleccoinar las variables de ventas y las evaluaciones de cada videojuego. Para analizar el comportamiento vamos a excluir ventas globales ya que es una variable linealmente dependiente del resto de las ventas. 


Antes de clusterizar debemos preparar la data:
- Eliminando datos faltantes. 

- Pasar User_score a numerico.

- Escalar la data

```{r }


data$User_Score <- as.numeric(data$User_Score)

data <- data %>% filter(!(is.na(Critic_Score) | is.na(User_Score))) %>% select(-Global_Sales)

data_numerica <- data[,c(6:9, 10, 12)]

data_escala = scale(data_numerica) %>% as_tibble()

data_escala %>% summary()

```

Ya tenemos escalada la data, vamos a aplicar el algoritmo de kmedias, que viene implementado en R base. 
Para probar, vamos a aplicar kmedias con k = 10

```{r}

modelo_kmeans <- kmeans(data_escala, centers = 10)

# creo la variable cluster en la tabla data_escalada
data_escala$clus <- modelo_kmeans$cluster %>% as.factor()

ggplot(data_escala, aes(Critic_Score, User_Score, color=clus)) +
  geom_point(alpha=0.5, show.legend = F) +
  theme_bw()

```

Vamos a ver como evoluciona la suma de cuadrados intra-cluster en la medida que aumentamos el numero de k


```{r}
SSinterior <- numeric(50)

for(k in 1:50){
  modelo <- kmeans(data_escala, centers = k)
  SSinterior[k] <- modelo$tot.withinss
}

plot(SSinterior)

```


## Evaluacion

Existen diversos metodos de evaluacion de calidad de los clusters resultantes. El primero que revisaremos es la inspeccion visual

```{r}

# uso distancia euclidiana
tempDist <- dist(data_numerica) %>% as.matrix()

#reordeno filas y columnas en base al cluster obtenido
index <- sort(modelo_kmeans$cluster, index.return=TRUE)
tempDist <- tempDist[index$ix,index$ix]
rownames(tempDist) <- c(1:nrow(data))
colnames(tempDist) <- c(1:nrow(data))

image(tempDist)



```



El siguiente metodo es el estadistico de Hopkins, que esta implementado en la libreria factoextra. 


```{r}
library(factoextra)

#Calcula el hopkins statistic 
res <- get_clust_tendency(data_numerica, n = 30, graph = FALSE)

print(res)

```
Luego vamos a implementar el indice de correlacion

```{r}



#Correlation
#construyo matriz de correlacion ideal (cada entidad correlaciona 1 con su cluster)
tempMatrix <- matrix(0, nrow = nrow(data_escala), ncol = nrow(data_escala))
tempMatrix[which(index$x==1), which(index$x==1)]  <- 1
tempMatrix[which(index$x==2), which(index$x==2)]  <- 1
tempMatrix[which(index$x==3), which(index$x==3)]  <- 1
tempMatrix[which(index$x==3), which(index$x==4)]  <- 1
tempMatrix[which(index$x==3), which(index$x==5)]  <- 1
tempMatrix[which(index$x==3), which(index$x==6)]  <- 1
tempMatrix[which(index$x==3), which(index$x==7)]  <- 1
tempMatrix[which(index$x==3), which(index$x==8)]  <- 1
tempMatrix[which(index$x==3), which(index$x==9)]  <- 1
tempMatrix[which(index$x==3), which(index$x==10)] <- 1

#construyo matriz de disimilitud
tempDist2 <- 1/(1+tempDist)

#Calcula correlacion 
cor <- cor(tempMatrix[upper.tri(tempMatrix)],tempDist2[upper.tri(tempDist2)])

print(cor)

```

Tambien implementaremos indice de cohesion y el de separacion, que son muy similares.

```{r}
library(flexclust) # usaremos la distancia implementada en flexclus (dist2) que maneja mejor objetos de diferrente tamaÃ±o
data_escala <- apply(data_escala,2,as.numeric)
 
#Cohesion
withinCluster <- numeric(10)
for (i in 1:10){
  tempData <- data_escala[which(modelo_kmeans$cluster == i),]
  withinCluster[i] <- sum(dist2(tempData,colMeans(tempData))^2)
}
cohesion = sum(withinCluster)
#es equivalente a model$tot.withinss en k-means
print(c(cohesion, modelo_kmeans$tot.withinss))

#Separation
meanData <- colMeans(data_escala)
SSB <- numeric(10)
for (i in 1:10){
  tempData <- data_escala[which(modelo_kmeans$cluster==i),]
  SSB[i] <- nrow(tempData)*sum((meanData-colMeans(tempData))^2)
}
separation = sum(SSB)

print(separation)

```
Y finalmente aplicamos el coeficiente de silueta, implementado en libreria cluser

```{r}
library(cluster)

coefSil <- silhouette(modelo_kmeans$cluster,dist(data_escala))
summary(coefSil)

#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSil) + coord_flip()
```

# Utilizamos el coeficiente de silueta para encontrar el mejor valor de K

```{r}

coefSil=numeric(30)
for (k in 2:30){
  modelo <- kmeans(data_escala, centers = k)
  temp <- silhouette(modelo$cluster,dist(data_escala))
  coefSil[k] <- mean(temp[,3])
}
tempDF=data.frame(CS=coefSil,K=c(1:30))

ggplot(tempDF, aes(x=K, y=CS)) + 
  geom_line() +
  scale_x_continuous(breaks=c(1:30))

```

