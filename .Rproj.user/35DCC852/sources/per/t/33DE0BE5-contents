---
title: "Clusters probabilisticos"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Para este ejercicio seguiremos utilizando los datos de videojuegos que hemos usado en sesiones anteriores, pero solo utilizaremos las variables User Score y Critic Score para visualizar mejor los resultados en un grafico 2D

```{r, message = FALSE, warning = FALSE}

library(tidyverse)

data_escalada  <- read.csv("video_games_sales.csv") %>% 
  mutate(User_Score = as.numeric(User_Score)) %>% 
  filter(!(is.na(Critic_Score) | is.na(User_Score))) %>% 
  select(-Global_Sales) %>% 
  select(Critic_Score, User_Score) %>% 
  scale() %>% 
  as_tibble()

data_escalada %>% summary()

```

El primer metodo que implementaremos es el clustering basado en densidad, que esta implementado en la libreria DBSCAN

```{r, warning = FALSE, message = FALSE}

library(dbscan)

model = dbscan(data_escalada, eps = 0.1, minPts = 15)

model

```

Se puede ver que el modelo genero 16 clusters basado en los parametros minPts y eps que le entregamos a la funcion dbscan

En la figura a continuacion podemos ver que los clusters estan repartidos por el espacio

```{r}
ggplot(data_escalada, aes(Critic_Score, User_Score, color = factor(model$cluster))) + 
  geom_point(alpha = 0.3) 

```

Se puede ver que hay diversos puntos que no quedan asignados a ningun cluster dados los valores escogidos para la distancia minima. 

Otros algoritmos como el c-means permiten asignarle un cluster a todos los puntos

Para aplicar cmeans utilizaremos una libreria llama e1071, aunque hay otras implementaciones

```{r}

library(e1071)

modelo_c_means <- cmeans(data_escalada, 16, m=3) 

modelo_c_means$membership %>% head()



```

El algoritmo cmeans asigna como cluster al que tenga mayor probabilidad

```{r}
ggplot(data_escalada, aes(Critic_Score, User_Score, color = factor(modelo_c_means$cluster))) + 
  geom_point(alpha = 0.3) 
```

Para los modelos de clustering difuso podemos calcular el Coeficiente de partición difusa (FPC) 

```{r}

matriz <- modelo_c_means$membership%*%t(modelo_c_means$membership) # producto matricial

(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))


```

El valor del FPC es bajo, lo que significa que los grupos tienen alta variabilidad, y se puede confirmar en la figura ya que no se ven grupos definidos.

Al igual que en cmeans, los metodos de GMM permiten obtener clusters difusos pero utilizando modelos probabilisticos. 

Para aplicar GMM, utilizamos la libreria mclust

```{r, message = FALSE}

library(mclust)

model_gmm = Mclust(data_escalada)

model_gmm 
summary(model_gmm, parameters = TRUE)


```

El modelo genero 5 clusters los que se pueden visualizar igual que los ejemplos anteriores

```{r}
ggplot(data_escalada) + 
  aes(x=Critic_Score, y=User_Score, color=factor(model_gmm$classification)) + 
  geom_point(alpha=0.5) 
```

El modelo aplicó todas las formas posibles de la matriz de covarianzas, y permite visualizar como evoluciona el BIC a medida que aumentamos el numero de clusters. Esta visualizacion permite ver que la mayoria de los modelos deja de mejorar sobre 5 clusters

```{r}
plot(model_gmm, what = "BIC")
```





